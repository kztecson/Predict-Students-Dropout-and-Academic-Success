---
title: "Predict Students' Dropout and Academic Success"
subtitle: "Group 5 Project"
author: "Ida Siewe, Haris Malik, Azamat Tukpatov, Kenth Tecson"
date: "2025-11-19"
format:
  pdf:
    toc: true
    code-fold: true
    theme: cosmo
engine: visual
---

```{r}
#| label: setup
#| include: false
#| echo: false

# Load Libraries
library(tidyverse)
library(randomForest)
library(rpart)
library(rpart.plot)

# Load Data
data <- read.csv("data.csv", sep = ";") 

# Convert Target to Factor
data$Target <- as.factor(data$Target)

# Handle any explicit missing values
data <- na.omit(data)
```

# Introduction

## Description of Data

Student retention is a significant challenge in higher education, making the identification of students at risk of dropping out crucial for institutions.

For our project, we are utilizing the "Predict Students' Dropout and Academic Success" dataset sourced from the UCI Machine Learning Repository. This dataset was developed by the Instituto Politécnico de Portalegre and contains **4,424 observations** with **36 variables**.

**Primary Research Question:** What factors most strongly influence a student's likelihood of dropping out or succeeding?

## Description of Variables

### Response Variable

-   **Target:** The student's academic status at the end of the normal duration of the course.
    -   This is a categorical variable with three classes: **Dropout**, **Enrolled**, and **Graduate**.

### Predictors

While the dataset contains 37 variables, we have selected the following key potential predictors for our analysis, focusing on academic performance, socioeconomic status, and demographics:

-   **Curricular units (1st & 2nd Sem):** Specifically, the grades and number of units approved, which serve as direct indicators of current academic performance.
-   **Tuition fees up to date:** Indicates if the student's tuition payments are current.
-   **Admission grade:** The grade used for admission.
-   **Previous qualification (grade):** The grade obtained in the student's qualification prior to enrollment.
-   **Age at enrollment:** The age of the student at the time of enrollment.
-   **Scholarship holder:** Indicates whether the student receives financial aid.
-   **Course:** The specific degree program the student is enrolled in.
-   **Parents' Qualification:** The highest education level of the student's mother and father.
-   **Nationality:** The nationality of the student.
-   **Marital status:** The marital status of the student.
-   **Application mode:** The method of application used by the student.

### Exploratory Analytics

The dataset includes 4,424 students and 37 total variables, based on dim(data) = 4424 × 37. The outcome variable, Target, separates students into three groups: Dropout (1,421 students, 32.1%), Enrolled (794 students, 17.9%), and Graduate (2,209 students, 49.9%). Since the Graduate group is much larger than the others, the data is noticeably unbalanced. This imbalance is something we need to keep in mind later when we build our models, because it can affect accuracy unless we use things like stratified sampling or class-balancing techniques.

```{r}
dim(data)
table(data$Target)
round(prop.table(table(data$Target)), 3)
```

```{r}

# standardize response
data$Status <- factor(data$Target, levels = c("Dropout","Enrolled","Graduate"))
cat_cols <- c("Marital.status","Application.mode","Course","Nationality",
              "Daytime.Evening.attendance","Scholarship.holder","Debtor",
              "Mother.s.qualification","Father.s.qualification","Tuition.fees.up.to.date")
for (c in intersect(cat_cols, names(data))) data[[c]] <- factor(data[[c]])

# numeric summaries
num_vars <- intersect(c("Admission.grade","Previous.qualification..grade.",
                        "Age.at.enrollment","Curricular.units.1st.sem..grade.",
                        "Curricular.units.2nd.sem..grade."), names(data))
if (length(num_vars)>0) print(summary(data[, num_vars]))




```

We standardized the response into Status = factor(Target) and coerced likely categorical predictors (e.g., marital status, course, scholarship, debtor) to factor type. Numeric summaries for key variables are as follows: Admission.grade (Min = 95.0, Q1 ≈ 117.9, Median = 126.1, Mean = 127.0, Q3 ≈ 134.8, Max = 190.0); Previous.qualification..grade. (Min = 95.0, Median ≈ 133.1, Mean ≈ 132.6); Age.at.enrollment (Min = 17, Q1 = 19, Median = 20, Mean ≈ 23.27, Max = 70); Curricular.units.1st.sem..grade. and Curricular.units.2nd.sem..grade. (medians ≈ 12.29 and 12.20, means ≈ 10.64 and 10.23 respectively). The small differences between means and medians for admission and previous grades indicate roughly symmetric distributions, whereas the lower means relative to medians for semester grades point to left-skewed distributions with a tail toward lower scores. The age distribution exhibits a right tail driven by older students (outliers). For modeling, we recommend treating extreme ages explicitly, considering robustness to outliers for grade variables, and possibly binning or transforming grades if needed.

```{r}
library(ggplot2)
ggplot(data, aes(x = Status)) +
  geom_bar(aes(y = (..count..)/sum(..count..)), fill="steelblue") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  geom_text(stat="count", aes(label = scales::percent(..count../sum(..count..), accuracy = 1),
                              y = (..count..)/sum(..count..)), vjust = -0.5) +
  theme_minimal() +
  labs(title="Student Status proportions", y="Proportion", x="")
```

After creating two combined performance measures—one counting how many curricular units each student successfully completed in their first year, and another summarizing their average grades across available academic records—I compared these values across the three status groups. A clear pattern showed up right away. Students who eventually graduated had the strongest first-year results, completing a little over twelve units on average and holding the highest median grades. Enrolled students fell into a middle zone with moderate course completion and grades. Meanwhile, the dropout group had noticeably lower numbers, passing fewer than five units during their first year and showing much lower median grades. This gap suggests that early academic performance is one of the clearest indicators of whether a student is likely to struggle, persist, or eventually graduate.

```{r}
data <- data %>%
  mutate(
    CU1_approved = ifelse(!is.na(Curricular.units.1st.sem..approved.), Curricular.units.1st.sem..approved., 0),
    CU2_approved = ifelse(!is.na(Curricular.units.2nd.sem..approved.), Curricular.units.2nd.sem..approved., 0),
    first_year_approved = CU1_approved + CU2_approved,
    avg_year_grade = rowMeans(select(., any_of(c("Curricular.units.1st.sem..grade.","Curricular.units.2nd.sem..grade.","Admission.grade","Previous.qualification..grade."))), na.rm = TRUE)
  )
# quick check
data %>% group_by(Status) %>% summarise(mean_first_year_approved = mean(first_year_approved, na.rm=TRUE),
                                        median_avg_grade = median(avg_year_grade, na.rm=TRUE))
```

# Methods and Results

## 1. Decision Tree Model (Azamat Tukpatov)

We are using a decision tree as our first model because it is simple, interpretable, and works well with a categorical response variable. Decision trees can capture nonlinear relationships between variables. But a single decision tree can have high variance and is not the best in terms of prediction accuracy.

## (a) Model formula

Status = Curricular Units (1st & 2nd Sem) + Tuition Fees Up to Date + Admission Grade + Previous Qualification Grade + Age at Enrollment + Scholarship Holder + Course + Parents' Qualification + Nationality + Marital Status + Application Mode + Debtor Status + Daytime/Evening Attendance

## (b) Thought process for fitting and pruning the decision tree

```{r}
library(tree)

students <- na.omit(
  data[, c(
    "Status",
    "Admission.grade",
    "Previous.qualification..grade.",
    "Age.at.enrollment",
    "Curricular.units.1st.sem..grade.",
    "Curricular.units.2nd.sem..grade.",
    "Curricular.units.1st.sem..approved.",
    "Curricular.units.2nd.sem..approved.",
    "Tuition.fees.up.to.date",
    "Scholarship.holder",
    "Debtor",
    "Daytime.evening.attendance.",
    "Marital.status"
  )]
)

tree_full <- tree(Status ~ ., data = students)

plot(tree_full)
text(tree_full, pretty = 0)

set.seed(10)
cv_tree <- cv.tree(tree_full, FUN = prune.misclass)

plot(cv_tree$size, cv_tree$dev, type = "b",
     xlab = "Tree size",
     ylab = "Deviance")

best_size <- cv_tree$size[which.min(cv_tree$dev)]
tree_pruned <- prune.misclass(tree_full, best = best_size)

plot(tree_pruned)
text(tree_pruned, pretty = 0)
```

The full tree had multiple terminal nodes and could potentially overfit, so we applied pruning. The cross validation showed that the subtree with 7 terminal nodes gave the lowest deviance, so we selected that as our final pruned tree.

### (c) Train/Test Evaluation

```{r}

set.seed(10)
tree_errors <- rep(0, 10)

for (i in 1:10) {
  
  train_index <- sample(1:nrow(students), size = 0.8 * nrow(students))
  train_set <- students[train_index, ]
  test_set  <- students[-train_index, ]
  
  tree_full <- tree(Status ~ ., data = train_set)
  
  cv_out <- cv.tree(tree_full, FUN = prune.misclass)
  best_size <- cv_out$size[which.min(cv_out$dev)]
  tree_pruned <- prune.misclass(tree_full, best = best_size)
  
  pred <- predict(tree_pruned, newdata = test_set, type = "class")
  conf_mat <- table(test_set$Status, pred)
  tree_errors[i] <- 1 - sum(diag(conf_mat)) / sum(conf_mat)
  
  print(paste("Iteration", i, "Test Error =", round(tree_errors[i], 4)))
}

tree_errors

mean(tree_errors)

```

We randomly split the data 10 times into 80% training and 20% testing. For each split, we fit and pruned a decision tree on the training set and computed misclassification rate on the test set. The average test error was 0.271, meaning the model achieves 72.9% test accuracy.

The decision tree mostly used academic performance to make predictions, especially second semester grades and how many courses passed. It also used tuition fees and age in some of the smaller splits. This shows that early academic progress and financial standing are the biggest factors that separate Dropout, Enrolled, and Graduate students.

## 2. Random Forest Model Training and Cross-Validation (Kenth Tecson)

## (a) Model formula

## $$Status \sim . - Target$$

Predict Status using all variables in the dataset. Exclude the original string Target variable to avoid redundancy.

## (b) Thought process

We selected the Random Forest model to complement the Decision Tree. While the single decision tree offers high interpretability, it can suffer from high variance and be prone to overfitting. Random Forest addresses these limitations by creating an ensemble of decorrelated trees, resulting in higher predictive accuracy. Unlike the Decision Tree model where we manually selected a subset of predictors, for the Random Forest we decided to utilize all available predictors in the dataset. This allows the model to identify feature importance and capture non linear interactions between the demographic, socioeconomic, and academic variables without manual filtering. We used the standard configuration for the number of trees ($ntree=500$) to ensure the error rate stabilizes.

## (c) Train/Test Evaluation

We performed a 10 fold cross validation. In each iteration, the dataset was randomly split into 80% training and 20% testing sets using set.seed(10) to ensure reproducibility. The model is trained on the training set and calculates the misclassification error on the test set.

```{r}
#| label: random-forest-cv
#| warning: false
#| message: false

rf_data <- data %>% select(-Target)

set.seed(10)
rf_errors <- numeric(10)

for(i in 1:10){
  train_idx <- sample(1:nrow(rf_data), size = 0.8 * nrow(rf_data))
  train_set <- rf_data[train_idx, ]
  test_set  <- rf_data[-train_idx, ]
  
  # Fit Random Forest
  rf_model <- randomForest(Status ~ ., data = train_set, importance = TRUE)
  
  # Predict on Test Set
  rf_pred <- predict(rf_model, newdata = test_set)
  
  # Calculate Misclassification Error
  rf_errors[i] <- mean(rf_pred != test_set$Status)
}

mean_rf_error <- mean(rf_errors)
print(paste("Mean Test Error Rate:", round(mean_rf_error, 4)))
```

The Random Forest model achieved a mean test error rate of 22.05% (Accuracy 77.95%). This is a significant improvement over the single Decision Tree model, which had an error rate of 27.1%, confirming that the ensemble approach effectively reduces variance and captures more signal from the data.

## Variable Importance

We examined the variable importance plot to understand which factors drove these predictions.

```{r}
#| label: rf-importance-plot
#| echo: false

varImpPlot(rf_model, main = "Random Forest Variable Importance")
```

The importance plot shows that the variables related to academic performance are the most critical predictors of student success. Curricular units 2nd sem (approved & grade) and Curricular units 1st sem (approved & grade) are the dominant features, suggesting that a student's performance in their very first year is the strongest indicator of whether they will eventually dropout or graduate. Tuition fees up to date is also a highly significant predictor, highlighting the impact of financial stability on student retention. Age at enrollment and admission grades are also shown to contribute moderately but are less significant than the actual university grades.

# Conclusion (Haris Malik)

## Model Comparison

Our analysis successfully identified the key factors that influence student dropout and academic success. To compare the performance of both models, we summarized their key metrics:

```{r}
#| label: model-comparison
#| echo: false

library(knitr)

comparison_df <- data.frame(
  Model = c("Decision Tree (Pruned)", "Random Forest"),
  Test_Error = c("27.1%", "22.05%"),
  Test_Accuracy = c("72.9%", "77.95%"),
  Interpretability = c("High", "Low"),
  Key_Predictors = c("2nd sem grades, tuition fees, age", "1st & 2nd sem grades, tuition fees")
)

kable(comparison_df, 
      col.names = c("Model", "Test Error", "Test Accuracy", "Interpretability", "Key Predictors"),
      caption = "Comparison of Decision Tree and Random Forest Performance")
```

Both models pointed to the same conclusion: early academic performance matters most. Students who pass more courses in their first year and maintain higher grades are significantly more likely to graduate, while those struggling early on face much higher dropout risk.

The Random Forest model achieved better accuracy (77.95%) compared to the Decision Tree (72.9%), which makes sense given that ensemble methods reduce overfitting. However, the Decision Tree was easier to interpret and clearly showed that second semester grades and tuition fee status were the main decision points.

Beyond academic performance, financial stability played a major role. The tuition fees up to date variable appeared as an important predictor in both models, suggesting that students facing financial pressure are at higher risk of dropping out. Age at enrollment and admission grades had some effect but were less critical than actual university performance.

These findings have practical value for universities trying to improve retention. If institutions can identify struggling students early in their first semester and provide academic support or financial aid, they could prevent many dropouts before students fall too far behind. The data suggests that waiting until second year to intervene may be too late.

Our models had some limitations worth noting. The class imbalance in the dataset (50% graduates vs. 32% dropouts vs. 18% enrolled) may have affected prediction accuracy for the enrolled group. The dataset also doesn't include factors like work obligations or mental health that could influence dropout decisions. Future work could test other methods like gradient boosting or neural networks to see if accuracy improves further.

Overall, this project demonstrates that universities already have the data needed to predict which students are at risk. The challenge is using these insights to build effective early intervention programs that actually help students succeed.

# Bibliography

Realinho, V., Baptista, L., Machado, J., & Martins, M. V. (2021). *Predict Students' Dropout and Academic Success* \[Dataset\]. UCI Machine Learning Repository. <https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success>